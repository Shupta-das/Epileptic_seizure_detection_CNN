{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T17:45:30.86623Z","iopub.status.busy":"2023-09-24T17:45:30.865885Z","iopub.status.idle":"2023-09-24T17:45:56.186112Z","shell.execute_reply":"2023-09-24T17:45:56.184888Z","shell.execute_reply.started":"2023-09-24T17:45:30.866198Z"},"trusted":true},"outputs":[],"source":["!pip install EMD-signal\n","from scipy import stats\n","from scipy import fftpack\n","from PyEMD import EMD, EEMD # https://pyemd.readthedocs.io/en/latest/usage.html\n","from scipy.signal import find_peaks\n","from scipy.stats import kurtosis, skew\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import minmax_scale\n","import pandas as pd\n","import numpy as np\n","import statistics\n","import pickle\n","import math\n","import cmath\n","import os\n","import math \n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.layers import Dense, Activation,Conv2D,MaxPooling2D, Dropout, Flatten\n","from sklearn.model_selection import *\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support\n","import pandas as pd\n","import os\n","from sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_classif\n","from sklearn.neural_network import MLPClassifier\n","from numpy import mean\n"]},{"cell_type":"markdown","metadata":{},"source":["## IMF Creation"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T17:45:56.189442Z","iopub.status.busy":"2023-09-24T17:45:56.188247Z","iopub.status.idle":"2023-09-24T17:45:56.195702Z","shell.execute_reply":"2023-09-24T17:45:56.194584Z","shell.execute_reply.started":"2023-09-24T17:45:56.189411Z"},"trusted":true},"outputs":[],"source":["def cal_IMF(start, channel, data): \n","    # print(data.shape)\n","    sample_rate = 256\n","    seconds = 10\n","    num_samples = sample_rate*seconds\n","    time_vect = (data.iloc[start:start+num_samples, channel])\n","    time_vect = np.array(time_vect)\n","    emd = EMD()\n","    imfs = emd.emd(time_vect)\n","    return imfs"]},{"cell_type":"markdown","metadata":{},"source":["## Fluction Index Calculation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fluctuation_index(x):\n","    sum = 0\n","    # print(x.shape[0]-1)\n","    for i in range(0, x.shape[0]-1):\n","        sum+=abs(x[i+1]-x[i])\n","        # print(sum)\n","    return sum/(x.shape[0]-1)"]},{"cell_type":"markdown","metadata":{},"source":["## SODP Calculation"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T17:46:19.652676Z","iopub.status.busy":"2023-09-24T17:46:19.651721Z","iopub.status.idle":"2023-09-24T17:46:19.661354Z","shell.execute_reply":"2023-09-24T17:46:19.660355Z","shell.execute_reply.started":"2023-09-24T17:46:19.652607Z"},"trusted":true},"outputs":[],"source":["def calc_area_of_sodp(X,Y,i,channel):\n","        #Area of Second Order Difference Plot\n","        SX = math.sqrt(np.sum(np.multiply(X,X))/len(X))\n","        SY = math.sqrt(np.sum(np.multiply(Y,Y))/len(Y))\n","        SXY = np.sum(np.multiply(X,Y))/len(X)\n","        D = cmath.sqrt((SX*SX) + (SY*SY) - (4*(SX*SX*SY*SY - SXY*SXY)))\n","        # print(D)\n","        a = 1.7321 *cmath.sqrt(SX*SX + SY*SY + D)\n","        b = 1.7321 * cmath.sqrt(SX*SX + SY*SY - D)\n","        Area = math.pi *a *b\n","        # print(SX,SY,SXY, D, a, b, Area)\n","        # print(\"Channel=  \",channel,\"Area of SODP of IMF number= \",i, \" is \", Area.real, \" \", Area.imag)\n","        return Area.real"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T17:46:22.152937Z","iopub.status.busy":"2023-09-24T17:46:22.152219Z","iopub.status.idle":"2023-09-24T17:46:22.1619Z","shell.execute_reply":"2023-09-24T17:46:22.160976Z","shell.execute_reply.started":"2023-09-24T17:46:22.152893Z"},"trusted":true},"outputs":[],"source":["def SODP(y, i, channel):\n","        #remove outliers\n","        upper_quartile = np.percentile(y,80)\n","        lower_quartile = np.percentile(y,20)\n","        IQR = (upper_quartile - lower_quartile) * 1.5\n","        quartileSet = (lower_quartile- IQR, upper_quartile +IQR)\n","        y = y[np.where((y >= quartileSet[0]) & (y <= quartileSet[1]))]\n","        \n","        #plotting SODP\n","        # X = []\n","        # Y= []\n","        # for n in range(0, y.shape[0]-2):\n","        #     X.append(y[n+1]-y[n])\n","        #     Y.append(y[n+2]-y[n+1])\n","        X = np.subtract(y[1:],y[0:-1]) #x(n+1)-x(n)\n","        Y = np.subtract(y[3:],y[0:-3]).tolist()#x(n+2)-x(n-1)\n","        Y.extend([0])\n","        Y.extend([0])\n","        # plt.figure(figsize=(12, 6))\n","        # plt.scatter(X, Y, s=80, facecolors='none', edgecolors='b')\n","        # plt.show()\n","        # self.save_fig(X,Y,'.','SODP'+str(i),dir_+'/SODP'+str(i)+'.png')\n","        Area = calc_area_of_sodp(X,Y,i,channel) \n","        # print(Area)\n","        return Area"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T17:46:25.105376Z","iopub.status.busy":"2023-09-24T17:46:25.10495Z","iopub.status.idle":"2023-09-24T17:46:25.112116Z","shell.execute_reply":"2023-09-24T17:46:25.110822Z","shell.execute_reply.started":"2023-09-24T17:46:25.105341Z"},"trusted":true},"outputs":[],"source":["seg = 10\n","samp = 256\n","non_overlap = 3\n","overlap = 7\n","step = non_overlap*samp\n","dist=seg*samp"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T17:46:26.713519Z","iopub.status.busy":"2023-09-24T17:46:26.713135Z","iopub.status.idle":"2023-09-24T17:46:53.260599Z","shell.execute_reply":"2023-09-24T17:46:53.259441Z","shell.execute_reply.started":"2023-09-24T17:46:26.713489Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(2851328, 22)\n","3710\n"]}],"source":["\n","\n","# # file = open(\"/kaggle/input/chbmit-patient-wise-f1/merge_train_seizure.csv\")\n","# # ts=pickle.load(file)\n","# # file.close()\n","\n","\n","# data=pd.read_csv(\"/kaggle/input/prev-chbmit-raw-data-sc/merge_all_nonseizure.csv\",header=None)\n","\n","# print(data.shape)\n","# data_shape = data.shape[0]\n","# # print(data_shape)\n","# full_sec = (data_shape//samp-(overlap))//non_overlap\n","# print(full_sec)"]},{"cell_type":"markdown","metadata":{},"source":["## Seizure Feature Creation Start"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T17:49:01.253388Z","iopub.status.busy":"2023-09-24T17:49:01.252133Z","iopub.status.idle":"2023-09-24T17:49:26.04478Z","shell.execute_reply":"2023-09-24T17:49:26.043796Z","shell.execute_reply.started":"2023-09-24T17:49:01.253347Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2851328\n","3710\n"]}],"source":["final_feature=[]\n","seizure_data = pd.read_csv(\"/kaggle/input/prev-chbmit-raw-data-sc/merge_all_seizure.csv\",header=None)\n","seizure = seizure_data.shape[0]\n","print(seizure)\n","cnt =0\n","step = non_overlap*samp\n","for k in range (0,data_shape-(overlap*samp)-step,step):\n","# for k in range(0,2):\n","#     print(k//samp)\n","    cnt=cnt+1\n","    # print(cnt)\n","    feature = []\n","    ok = 1\n","    for i in range(0, 22):               \n","        imf = cal_IMF(k,i,seizure_data)\n","        imf = np.array(imf)\n","        if(imf.shape[0]<6):\n","            print(k,i,\"boom\")\n","            i=22\n","            ok = 0\n","            continue\n","        row = []\n","        for j in range(0,6):\n","            m = fluctuation_index(imf[j])\n","            n = statistics.variance(imf[j]) # entropy korte hbe\n","            o = SODP(imf[j], j, i)\n","            # final[k//step][i][3*j+0]=float(m)\n","            # final[k//step][i][3*j+1]=float(n)\n","            # final[k//step][i][3*j+2]=float(o)\n","            row.append(m)\n","            row.append(n)\n","            row.append(o)\n","        feature.append(row)\n","    if ok:\n","        feature=np.array(feature)\n","        final_feature.append(feature)\n","        print(\"final feature len \", len(final_feature))\n","                \n","print(cnt)"]},{"cell_type":"markdown","metadata":{},"source":["## Non Seizure Feature creation "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","seizure_data = pd.read_csv(\"/kaggle/input/prev-chbmit-raw-data-sc/merge_all_nonseizure.csv\",header=None)\n","seizure = seizure_data.shape[0]\n","print(seizure)\n","cnt =0\n","step = non_overlap*samp\n","for k in range (0,data_shape-(overlap*samp)-step,step):\n","# for k in range(0,2):\n","#     print(k//samp)\n","    cnt=cnt+1\n","    # print(cnt)\n","    feature = []\n","    ok = 1\n","    for i in range(0, 22):               \n","        imf = cal_IMF(k,i,seizure_data)\n","        imf = np.array(imf)\n","        if(imf.shape[0]<6):\n","            print(k,i,\"boom\")\n","            i=22\n","            ok = 0\n","            continue\n","        row = []\n","        for j in range(0,6):\n","            m = fluctuation_index(imf[j])\n","            n = statistics.variance(imf[j]) # entropy korte hbe\n","            o = SODP(imf[j], j, i)\n","            # final[k//step][i][3*j+0]=float(m)\n","            # final[k//step][i][3*j+1]=float(n)\n","            # final[k//step][i][3*j+2]=float(o)\n","            row.append(m)\n","            row.append(n)\n","            row.append(o)\n","        feature.append(row)\n","    if ok:\n","        feature=np.array(feature)\n","        final_feature.append(feature)\n","        print(\"final feature len \", len(final_feature))\n","                \n","print(cnt)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["final=np.array(final_feature)\n","final.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Adding Lable"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["temp = []\n","seizure = int(final.shape[0]/2)\n","for i in range(0, final.shape[0]):\n","    tt = final[i].reshape((396,))\n","    tt=tt.tolist()\n","    if(i<seizure):\n","        tt.append(1)\n","    else:\n","        tt.append(0)\n","    temp.append(tt)\n","temp=np.array(temp)\n","temp.shape"]},{"cell_type":"markdown","metadata":{},"source":["## 5 fold Split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv = StratifiedKFold(n_splits=5, random_state=42, shuffle = True)\n","\n","## data preprocessing\n","X = temp[:, :396]\n","y = temp[:,-1]\n","\n","\n","#####################################################################################\n","idx = 1\n","score = []\n","\n","for train_index, test_index in cv.split(X, y):\n","\n","    x_train,x_test = X[train_index],X[test_index]\n","    y_train,y_test = y[train_index],y[test_index]\n","#     x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1, stratify=y_train,random_state=42)\n","    \n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/x_train{idx}.txt\",'wb')\n","    pickle.dump(x_train, file)\n","    file.close()\n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/x_test{idx}.txt\",\"wb\")\n","    pickle.dump(x_test, file)\n","    file.close()\n","    \n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/y_train{idx}.txt\",\"wb\")\n","    pickle.dump(y_train, file)\n","    file.close()\n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/y_test{idx}.txt\",\"wb\")\n","    pickle.dump(y_test, file)\n","    file.close()\n","    \n","    \n","    idx = idx+1"]},{"cell_type":"markdown","metadata":{},"source":["## Model Train and Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for idx in range (1,6):\n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/x_train{idx}.txt\",\"rb\")\n","    x_train = pickle.load(file)\n","    file.close()\n","\n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/x_test{idx}.txt\",\"rb\")\n","    x_test = pickle.load( file)\n","    file.close()\n","\n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/y_train{idx}.txt\",\"rb\")\n","    y_train = pickle.load(file)\n","    file.close()\n","\n","    file = open(f\"/kaggle/input/5f-train-test-val-dataset/y_test{idx}.txt\",\"rb\")\n","    y_test = pickle.load( file)\n","    file.close()\n","    \n","    tx=[]\n","    for i in range (x_train.shape[0]):\n","        x1=x_train[i,:].reshape((22,18))\n","        tx.append(x1)\n","    x_train=np.asarray(tx)\n","    print(x_train.shape)\n","    tx=[]\n","    for i in range (x_test.shape[0]):\n","        x1=x_test[i,:].reshape((22,18))\n","        tx.append(x1)\n","    x_test=np.asarray(tx)\n","    print(x_test.shape)\n","\n","    y_train = to_categorical(y_train)\n","    y_test = to_categorical(y_test)\n","\n","    mi_max = 100\n","    acc_mx = -100\n","    k=\"nofs\"\n","    name = \"CNN2d50\"\n","    X_train_top = x_train\n","    X_test_top = x_test\n","\n","    #################################  Standard Scaler #####################\n","    from sklearn.preprocessing import StandardScaler\n","    for i in range(X_train_top.shape[0]):\n","        sc = StandardScaler()\n","        # X = sc.fit_transform(X)\n","        X_train_top[i] = sc.fit_transform(X_train_top[i])\n","        \n","    for i in range(X_test_top.shape[0]):\n","        sc = StandardScaler()\n","        # X = sc.fit_transform(X)\n","        X_test_top[i] = sc.fit_transform(X_test_top[i])\n","\n","    from keras.models import Sequential\n","    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n","\n","    model = Sequential()\n","    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(22,18,1)))\n","    model.add(MaxPooling2D())\n","    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n","    model.add(MaxPooling2D())\n","    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(50))\n","    model.add(Dense(2, activation='softmax'))\n","\n","    ################################# compile model #####################\n","    lr = .001\n","    batch_size = 128\n","    opt = tf.keras.optimizers.Adam(learning_rate = lr)\n","    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n","    ################################# fit model #####################\n","\n","    res_path = f'/kaggle/working/{name}_{idx}/mi_{k}/{idx}_MI_{k}.h5'\n","    mc = ModelCheckpoint(res_path, monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n","    history = model.fit(X_train_top,y_train,validation_data = (X_test_top, y_test),batch_size=batch_size,epochs=700,verbose=0,callbacks=[mc])\n","\n","    ################################# history data #####################\n","    train_loss=history.history['loss']\n","    val_loss=history.history['val_loss']\n","    accuracy=history.history['accuracy']\n","    val_accuracy=history.history['val_accuracy']\n","\n","\n","    ################################# dump history data #####################\n","    x = np.array(list(range(1,len(train_loss)+1)))\n","    loss = 'model1'\n","    datas = {'epoch':x, 'Training loss':train_loss}\n","    lossepoch = pd.DataFrame(datas)\n","    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_train_loss.csv', index = False)\n","\n","    datas = {'epoch':x, 'Test loss':val_loss}\n","    lossepoch = pd.DataFrame(datas)\n","    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_test_loss.csv', index = False)\n","\n","    datas = {'epoch':x, 'Training accuracy':accuracy}\n","    lossepoch = pd.DataFrame(datas)\n","    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_train_accuracy.csv', index = False)\n","\n","    datas = {'epoch':x, 'Test accuracy':val_accuracy}\n","    lossepoch = pd.DataFrame(datas)\n","    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_test_accuracy.csv', index = False)\n","\n","    ########################## performance evalution #############################\n","    model.load_weights(res_path)\n","    scores = model.evaluate(X_test_top, y_test, verbose=1, batch_size=batch_size)\n","    pred=model.predict(X_test_top, verbose=0, batch_size=batch_size)\n","    pred=np.round(pred)\n","    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","\n","    os.rename(f'/kaggle/working/{name}_{idx}/mi_{k}', f'/kaggle/working/{name}_{idx}/mi_{k}_{round(scores[1],4)}')\n","    acc_mx = round(scores[1],4)\n","    os.rename(f'/kaggle/working/{name}_{idx}', f'/kaggle/working/{name}_{idx}_{acc_mx}')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Result Store in excel Sheet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(val,index=id, columns=[f'F{idx}'])\n","print(df)\n","df.to_excel(f\"{name} f - {idx}.xlsx\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
