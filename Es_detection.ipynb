{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import library","metadata":{}},{"cell_type":"code","source":"!pip install EMD-signal\nfrom scipy import stats\nfrom scipy import fftpack\nfrom PyEMD import EMD, EEMD # https://pyemd.readthedocs.io/en/latest/usage.html\nfrom scipy.signal import find_peaks\nfrom scipy.stats import kurtosis, skew\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import minmax_scale\nimport pandas as pd\nimport numpy as np\nimport statistics\nimport pickle\nimport math\nimport cmath\nimport os\nimport math \nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.layers import Dense, Activation,Conv2D,MaxPooling2D, Dropout, Flatten\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nimport pandas as pd\nimport os\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_classif\nfrom sklearn.neural_network import MLPClassifier\nfrom numpy import mean\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:45:30.865885Z","iopub.execute_input":"2023-09-24T17:45:30.86623Z","iopub.status.idle":"2023-09-24T17:45:56.186112Z","shell.execute_reply.started":"2023-09-24T17:45:30.866198Z","shell.execute_reply":"2023-09-24T17:45:56.184888Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting EMD-signal\n  Downloading EMD_signal-1.5.1-py3-none-any.whl (74 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.12 in /opt/conda/lib/python3.10/site-packages (from EMD-signal) (1.23.5)\nRequirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.10/site-packages (from EMD-signal) (1.11.2)\nRequirement already satisfied: pathos>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from EMD-signal) (0.3.1)\nCollecting tqdm==4.64.* (from EMD-signal)\n  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos>=0.2.1->EMD-signal) (1.7.6.7)\nRequirement already satisfied: dill>=0.3.7 in /opt/conda/lib/python3.10/site-packages (from pathos>=0.2.1->EMD-signal) (0.3.7)\nRequirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos>=0.2.1->EMD-signal) (0.3.3)\nRequirement already satisfied: multiprocess>=0.70.15 in /opt/conda/lib/python3.10/site-packages (from pathos>=0.2.1->EMD-signal) (0.70.15)\nInstalling collected packages: tqdm, EMD-signal\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.1\n    Uninstalling tqdm-4.66.1:\n      Successfully uninstalled tqdm-4.66.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\nfitter 1.6.0 requires tqdm<5.0.0,>=4.65.1, but you have tqdm 4.64.1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed EMD-signal-1.5.1 tqdm-4.64.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## IMF Creation","metadata":{}},{"cell_type":"code","source":"def cal_IMF(start, channel, data): \n    # print(data.shape)\n    sample_rate = 256\n    seconds = 10\n    num_samples = sample_rate*seconds\n    time_vect = (data.iloc[start:start+num_samples, channel])\n    time_vect = np.array(time_vect)\n    emd = EMD()\n    imfs = emd.emd(time_vect)\n    return imfs","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:45:56.188247Z","iopub.execute_input":"2023-09-24T17:45:56.189442Z","iopub.status.idle":"2023-09-24T17:45:56.195702Z","shell.execute_reply.started":"2023-09-24T17:45:56.189411Z","shell.execute_reply":"2023-09-24T17:45:56.194584Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Fluction Index Calculation","metadata":{}},{"cell_type":"code","source":"def fluctuation_index(x):\n    sum = 0\n    # print(x.shape[0]-1)\n    for i in range(0, x.shape[0]-1):\n        sum+=abs(x[i+1]-x[i])\n        # print(sum)\n    return sum/(x.shape[0]-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SODP Calculation","metadata":{}},{"cell_type":"code","source":"def calc_area_of_sodp(X,Y,i,channel):\n        #Area of Second Order Difference Plot\n        SX = math.sqrt(np.sum(np.multiply(X,X))/len(X))\n        SY = math.sqrt(np.sum(np.multiply(Y,Y))/len(Y))\n        SXY = np.sum(np.multiply(X,Y))/len(X)\n        D = cmath.sqrt((SX*SX) + (SY*SY) - (4*(SX*SX*SY*SY - SXY*SXY)))\n        # print(D)\n        a = 1.7321 *cmath.sqrt(SX*SX + SY*SY + D)\n        b = 1.7321 * cmath.sqrt(SX*SX + SY*SY - D)\n        Area = math.pi *a *b\n        # print(SX,SY,SXY, D, a, b, Area)\n        # print(\"Channel=  \",channel,\"Area of SODP of IMF number= \",i, \" is \", Area.real, \" \", Area.imag)\n        return Area.real","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:46:19.651721Z","iopub.execute_input":"2023-09-24T17:46:19.652676Z","iopub.status.idle":"2023-09-24T17:46:19.661354Z","shell.execute_reply.started":"2023-09-24T17:46:19.652607Z","shell.execute_reply":"2023-09-24T17:46:19.660355Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def SODP(y, i, channel):\n        #remove outliers\n        upper_quartile = np.percentile(y,80)\n        lower_quartile = np.percentile(y,20)\n        IQR = (upper_quartile - lower_quartile) * 1.5\n        quartileSet = (lower_quartile- IQR, upper_quartile +IQR)\n        y = y[np.where((y >= quartileSet[0]) & (y <= quartileSet[1]))]\n        \n        #plotting SODP\n        # X = []\n        # Y= []\n        # for n in range(0, y.shape[0]-2):\n        #     X.append(y[n+1]-y[n])\n        #     Y.append(y[n+2]-y[n+1])\n        X = np.subtract(y[1:],y[0:-1]) #x(n+1)-x(n)\n        Y = np.subtract(y[3:],y[0:-3]).tolist()#x(n+2)-x(n-1)\n        Y.extend([0])\n        Y.extend([0])\n        # plt.figure(figsize=(12, 6))\n        # plt.scatter(X, Y, s=80, facecolors='none', edgecolors='b')\n        # plt.show()\n        # self.save_fig(X,Y,'.','SODP'+str(i),dir_+'/SODP'+str(i)+'.png')\n        Area = calc_area_of_sodp(X,Y,i,channel) \n        # print(Area)\n        return Area","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:46:22.152219Z","iopub.execute_input":"2023-09-24T17:46:22.152937Z","iopub.status.idle":"2023-09-24T17:46:22.1619Z","shell.execute_reply.started":"2023-09-24T17:46:22.152893Z","shell.execute_reply":"2023-09-24T17:46:22.160976Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"seg = 10\nsamp = 256\nnon_overlap = 3\noverlap = 7\nstep = non_overlap*samp\ndist=seg*samp","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:46:25.10495Z","iopub.execute_input":"2023-09-24T17:46:25.105376Z","iopub.status.idle":"2023-09-24T17:46:25.112116Z","shell.execute_reply.started":"2023-09-24T17:46:25.105341Z","shell.execute_reply":"2023-09-24T17:46:25.110822Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n\n# # file = open(\"/kaggle/input/chbmit-patient-wise-f1/merge_train_seizure.csv\")\n# # ts=pickle.load(file)\n# # file.close()\n\n\n# data=pd.read_csv(\"/kaggle/input/prev-chbmit-raw-data-sc/merge_all_nonseizure.csv\",header=None)\n\n# print(data.shape)\n# data_shape = data.shape[0]\n# # print(data_shape)\n# full_sec = (data_shape//samp-(overlap))//non_overlap\n# print(full_sec)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:46:26.713135Z","iopub.execute_input":"2023-09-24T17:46:26.713519Z","iopub.status.idle":"2023-09-24T17:46:53.260599Z","shell.execute_reply.started":"2023-09-24T17:46:26.713489Z","shell.execute_reply":"2023-09-24T17:46:53.259441Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(2851328, 22)\n3710\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Seizure Feature Creation Start","metadata":{}},{"cell_type":"code","source":"final_feature=[]\nseizure_data = pd.read_csv(\"/kaggle/input/prev-chbmit-raw-data-sc/merge_all_seizure.csv\",header=None)\nseizure = seizure_data.shape[0]\nprint(seizure)\ncnt =0\nstep = non_overlap*samp\nfor k in range (0,data_shape-(overlap*samp)-step,step):\n# for k in range(0,2):\n#     print(k//samp)\n    cnt=cnt+1\n    # print(cnt)\n    feature = []\n    ok = 1\n    for i in range(0, 22):               \n        imf = cal_IMF(k,i,seizure_data)\n        imf = np.array(imf)\n        if(imf.shape[0]<6):\n            print(k,i,\"boom\")\n            i=22\n            ok = 0\n            continue\n        row = []\n        for j in range(0,6):\n            m = fluctuation_index(imf[j])\n            n = statistics.variance(imf[j]) # entropy korte hbe\n            o = SODP(imf[j], j, i)\n            # final[k//step][i][3*j+0]=float(m)\n            # final[k//step][i][3*j+1]=float(n)\n            # final[k//step][i][3*j+2]=float(o)\n            row.append(m)\n            row.append(n)\n            row.append(o)\n        feature.append(row)\n    if ok:\n        feature=np.array(feature)\n        final_feature.append(feature)\n        print(\"final feature len \", len(final_feature))\n                \nprint(cnt)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:49:01.252133Z","iopub.execute_input":"2023-09-24T17:49:01.253388Z","iopub.status.idle":"2023-09-24T17:49:26.04478Z","shell.execute_reply.started":"2023-09-24T17:49:01.253347Z","shell.execute_reply":"2023-09-24T17:49:26.043796Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2851328\n3710\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Non Seizure Feature creation ","metadata":{}},{"cell_type":"code","source":"final_feature=[]\nseizure_data = pd.read_csv(\"/kaggle/input/prev-chbmit-raw-data-sc/merge_all_nonseizure.csv\",header=None)\nseizure = seizure_data.shape[0]\nprint(seizure)\ncnt =0\nstep = non_overlap*samp\nfor k in range (0,data_shape-(overlap*samp)-step,step):\n# for k in range(0,2):\n#     print(k//samp)\n    cnt=cnt+1\n    # print(cnt)\n    feature = []\n    ok = 1\n    for i in range(0, 22):               \n        imf = cal_IMF(k,i,seizure_data)\n        imf = np.array(imf)\n        if(imf.shape[0]<6):\n            print(k,i,\"boom\")\n            i=22\n            ok = 0\n            continue\n        row = []\n        for j in range(0,6):\n            m = fluctuation_index(imf[j])\n            n = statistics.variance(imf[j]) # entropy korte hbe\n            o = SODP(imf[j], j, i)\n            # final[k//step][i][3*j+0]=float(m)\n            # final[k//step][i][3*j+1]=float(n)\n            # final[k//step][i][3*j+2]=float(o)\n            row.append(m)\n            row.append(n)\n            row.append(o)\n        feature.append(row)\n    if ok:\n        feature=np.array(feature)\n        final_feature.append(feature)\n        print(\"final feature len \", len(final_feature))\n                \nprint(cnt)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final=np.array(final_feature)\nfinal.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding Lable","metadata":{}},{"cell_type":"code","source":"temp = []\nseizure = int(final.shape[0]/2)\nfor i in range(0, final.shape[0]):\n    tt = final[i].reshape((396,))\n    tt=tt.tolist()\n    if(i<seizure):\n        tt.append(1)\n    else:\n        tt.append(0)\n    temp.append(tt)\ntemp=np.array(temp)\ntemp.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature selection using MI","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, random_state=42, shuffle = True)\n\n## data preprocessing\nX = temp[:, :396]\ny = temp[:,-1]\n\n\n#####################################################################################\nidx = 1\nscore = []\n\nfor train_index, test_index in cv.split(X, y):\n\n    x_train,x_test = X[train_index],X[test_index]\n    y_train,y_test = y[train_index],y[test_index]\n#     x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1, stratify=y_train,random_state=42)\n    \n    file = open(f\"/kaggle/input/5f-train-test-val-dataset/x_train{idx}.txt\",'wb')\n    pickle.dump(x_train, file)\n    file.close()\n    file = open(f\"/kaggle/input/5f-train-test-val-dataset/x_test{idx}.txt\",\"wb\")\n    pickle.dump(x_test, file)\n    file.close()\n    \n    file = open(f\"/kaggle/input/5f-train-test-val-dataset/y_train{idx}.txt\",\"wb\")\n    pickle.dump(y_train, file)\n    file.close()\n    file = open(f\"/kaggle/input/5f-train-test-val-dataset/y_test{idx}.txt\",\"wb\")\n    pickle.dump(y_test, file)\n    file.close()\n    \n    \n    ################################# Mutual info based best feature selection #####################\n\n    # Compute mutual information between each feature and the target variable for the training set\n    mutual_info = mutual_info_classif(x_train, y_train)\n\n    # Sort the features by mutual information in descending order\n    indices = np.argsort(mutual_info)[::-1]\n    file = open(f\"/kaggle/input/5f-train-test-val-dataset/{idx}f_feature_matrix.txt\",\"wb\")\n    pickle.dump(indices, file)\n    file.close()\n    \n    idx = idx+1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Train and Test","metadata":{}},{"cell_type":"code","source":"\n####################################### init var #####################################################\n\nidx=4\nhn = 50\nname = f\"NN({hn})\"\nmi_mx = 100\nacc_mx = -100\nid = []\nval = []\n\n#################################train test val read for idx 1#########################################\n###################         x            ###########################\nfile = open(f\"/kaggle/input/5f-train-test-val-dataset/x_train{idx}.txt\",\"rb\")\nx_train = pickle.load(file)\nfile.close()\n\nfile = open(f\"/kaggle/input/5f-train-test-val-dataset/x_test{idx}.txt\",\"rb\")\nx_test = pickle.load( file)\nfile.close()\n\n###################         y            ###########################\nfile = open(f\"/kaggle/input/5f-train-test-val-dataset/y_train{idx}.txt\",\"rb\")\ny_train = pickle.load(file)\nfile.close()\n\nfile = open(f\"/kaggle/input/5f-train-test-val-dataset/y_test{idx}.txt\",\"rb\")\ny_test = pickle.load( file)\nfile.close()\n\n\nfile = open(f\"/kaggle/input/5f-train-test-val-dataset/{idx}f_feature_matrix.txt\",\"rb\")\nindices = pickle.load( file)\nfile.close()\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n######################################################################################################################################\n\nfor k in range (250, 397):\n    \n    id.append(k)\n\n    top_features = indices[:k]\n    X_train_top = x_train[:, top_features]\n    X_test_top = x_test[:, top_features]\n    print(X_train_top.shape)\n    \n    #################################  Standard Scaler #####################\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    # X = sc.fit_transform(X)\n    X_train_top = sc.fit_transform(X_train_top)\n    X_test_top = sc.fit_transform(X_test_top)\n\n    ################################### reshape ##########################\n    X_train_top=X_train_top.reshape(X_train_top.shape[0],X_train_top.shape[1],1)\n    X_test_top=X_test_top.reshape(X_test_top.shape[0],X_test_top.shape[1],1)\n   \n    ################################# build model #####################\n    from keras.models import Sequential\n    from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization\n\n    model = Sequential()\n    model.add(Dense(hn, input_dim=X_train_top.shape[1], activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n\n\n\n    ################################# compile model #####################\n    lr = .001\n    batch_size = 128\n    opt = tf.keras.optimizers.Adam(learning_rate = lr)\n    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n    ################################# fit model #####################\n\n    res_path = f'/kaggle/working/{name}_{idx}/mi_{k}/{idx}_MI_{k}.h5'\n    mc = ModelCheckpoint(res_path, monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20)\n    history = model.fit(X_train_top,y_train,validation_data = (X_test_top, y_test),batch_size=batch_size,epochs=400,verbose=0,callbacks=[mc])\n\n    ################################# history data #####################\n    train_loss  = history.history['loss']\n    val_loss    = history.history['val_loss']\n    accuracy    = history.history['accuracy']\n    val_accuracy= history.history['val_accuracy']\n\n\n    ################################# dump history data #####################\n    x = np.array(list(range(1,len(train_loss)+1)))\n    loss = 'model1'\n    datas = {'epoch':x, 'Training loss':train_loss}\n    lossepoch = pd.DataFrame(datas)\n    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_train_loss.csv', index = False)\n\n    datas = {'epoch':x, 'Test loss':val_loss}\n    lossepoch = pd.DataFrame(datas)\n    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_test_loss.csv', index = False)\n\n    datas = {'epoch':x, 'Training accuracy':accuracy}\n    lossepoch = pd.DataFrame(datas)\n    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_train_accuracy.csv', index = False)\n\n    datas = {'epoch':x, 'Test accuracy':val_accuracy}\n    lossepoch = pd.DataFrame(datas)\n    lossepoch.to_csv(f'/kaggle/working/{name}_{idx}/mi_{k}/{loss}_test_accuracy.csv', index = False)\n\n\n    ########################## performance evalution #############################\n    model.load_weights(res_path)\n    scores = model.evaluate(X_test_top, y_test, verbose=1, batch_size=batch_size)\n    pred=model.predict(X_test_top, verbose=0, batch_size=batch_size)\n    pred=np.round(pred)\n    #         error = 0\n    #         for i in range (len(y_test)):\n    #             # print(test_Y[i][0], \" \", test_Y[i][1], \"--> \", pred[i][0],\" \", pred[i][1])\n    #             if y_test[i][0]!=pred[i][0] or test_Y[i][1]!=pred[i][1]:\n    #                 error = error+1\n    #         print(\"error -> \",error)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    val.append(scores[1])\n    os.rename(f'/kaggle/working/{name}_{idx}/mi_{k}', f'/kaggle/working/{name}_{idx}/mi_{k}_{round(scores[1],4)}')\n    if (round(scores[1],4)>acc_mx):\n        mi_mx = k\n        acc_mx = round(scores[1],4)\n    \nos.rename(f'/kaggle/working/{name}_{idx}', f'/kaggle/working/{name}_{idx}_{mi_mx}_{acc_mx}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result Store in excel Sheet","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(val,index=id, columns=[f'F{idx}'])\nprint(df)\ndf.to_excel(f\"{name} f - {idx}.xlsx\")","metadata":{},"execution_count":null,"outputs":[]}]}